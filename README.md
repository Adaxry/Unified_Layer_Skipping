# Unified_Layer_Skipping

* [Overview](#overview)
* [Requirements](#requirements)
* [Quick to Use](#quick-to-use)
* [Citation](#citation)
* [Contact](#contact)


## Overview
<p align="center">
  <img src="https://github.com/Adaxry/Unified_Layer_Skipping/blob/main/figures/overview.png" alt="overview" width="600"/>
</p>
<p align="center">
  Overview comparisions of serveral related approaches.
</p>


We propose a Unified Layer Skipping strategy for Large Language Models that selects and skips computational layers based on the target speedup ratio, providing stable acceleration, preserving performance, and supporting popular acceleration techniques, thereby outperforming existing dynamic computation methods in both inference performance and actual model throughput.

Details usages and commands for SFT and decoding will be uploaded within a week.

## Quick to Use

+ Fine-Tuning

+ Decoding



## Requirements
+ transformers>=4.28.0.dev0+
+ python>=3.8.0
+ torch>=1.10
+ deepspeed>=0.8.3+
+ datasets>=2.9.0+


## Contact
Please feel free to contact us (yijinliu@tencent.com) for any further questions.  
